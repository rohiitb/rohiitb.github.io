<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>Rohit Bhikule</title>
  
  <meta name="author" content="Rohit Bhikule">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/upenn_logo.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:43%;vertical-align:middle">
              <p style="text-align:center">
                <name>Rohit Bhikule</name>
              </p>
              <p> I'm a graduate student at University of Pennsylvania majoring in Robotics. I have had the opportunity to be a part of multiple projects affiliated to <a href="https://www.grasp.upenn.edu/">GRASP</a> robotics lab at UPenn. My projects have exposed me to various topics related to motion-planning, perception, computer vision and controls.<br>
                <br>
                My passion for robotics stems from my love for building and bringing things to life, and I am excited to continue exploring the possibilities in this field. 
                <br>
                Being a Mechanical engineer, I am a hands-on individual who likes working on practical projects, and at the same time, I also enjoy the challenge of coding and programming. 
                This combination of skills allows me to approach projects from both a theoretical and practical perspective, making me a well-rounded individual in the field of Robotics and software development.
                <br><br>
                In my leisure time, I like to go on hikes and play soccer. I am also a big motorsports racing fan.<br>
                Please feel free to connect with me with any questions or even if you want to have a fun gokart race with me.<br><br>
                Connect with me on <a href="https://www.linkedin.com/in/rbhikule/">LinkedIn</a>.
                
              </p>
              <p style="text-align:center">
                Email  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 
                Resume &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 
                GitHub
              </p>
              <p style="text-align:center">
                
                <a href="mailto:rbhikule@seas.upenn.edu"><img width="50" height="50" alt="mail icon" src="images/mail_icon.png" class="hoverZoomLink"></a> &nbsp &nbsp &nbsp &nbsp &nbsp
                <a href="data/Rohit_Bhikule_Resume.pdf"><img width="50" height="50" alt="resume icon" src="images/res_icon.png" class="hoverZoomLink"></a> &nbsp &nbsp &nbsp &nbsp &nbsp
                <a href="https://github.com/rohiitb/"><img width="50" height="50" alt="git icon" src="images/git_icon.png" class="hoverZoomLink"></a> 
              </p>
<!--               <p style="text-align:center">
                <a href="mailto:rbhikulej@seas.upenn.edu">Email</a> &nbsp/&nbsp
                <a href="data/Rohit_Bhikule_Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/rohiitb">github</a> &nbsp
              </p> -->
            </td>
            <td style="padding:2.5%;width:100%;max-width:120%">
              <a href="images/profile_pic.jpg"><img style="width:100%;max-width:150%" alt="profile photo" src="images/profile_pic.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        
<!--         Education starts here-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
              <tr>
              <td style="paddi0px;width:100%;vrtical-align:middle">
                <heading>Education</heading>
              </td>
            </tr>
          </tbody>
        </table>
        <br>
        
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
              <tr>
              <td style="paddi0px;width:100%;vrtical-align:middle">
                <papertitle>University of Pennsylvania</papertitle>
              </td>
              <td style="paddi0px;width:100%;vrtical-align:middle">
                August 2021 - May 2023
              </td>              
            </tr>
          </tbody>
        </table>
        
<!--         <papertitle>University of Pennsylvania</papertitle>
        <p style="text-align:right">August 2021 - May 2023</p>
        <br> -->
        
        <br><br><br>
        
      <!--         Experience starts here-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
              <tr>
              <td style="paddi0px;width:100%;vrtical-align:middle">
                <heading>Experience</heading>
              </td>
            </tr>
          </tbody>
        </table>
        <br><br><br>
        
        
        
        <br><br><br>
        
        <!-- Projects start here !-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
              <tr>
              <td style="paddi0px;width:100%;vrtical-align:middle">
                <heading>Projects</heading>
              </td>
            </tr>
          </tbody>
        </table>
        
        
        <table style="width:130%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
<!--                   Rebar Detection -->

                   <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"> 
                     <td style="padding:20px;width:30%;vertical-align:middle">
                      <div class="one">
                          <img src='images/reb_gif.gif' width="240" height="200">
                      </div>
                       
                       <script type="text/javascript">
                         function nerf_start() {
                           document.getElementById('nerf_image').style.opacity = "1";
                         }

                         function nerf_stop() {
                           document.getElementById('nerf_image').style.opacity = "0";
                         }
                         nerf_stop()
                       </script>
                     </td>
                     
                     
                     <td style="padding:20px;width:75%;vertical-align:middle">
                       <papertitle>
                         Rebar Intersection detection and tracking (Skymul, Atlanta)
                        </papertitle>
                       <br>
                      
                 <em>Summer 2022 (Internship)</em>
                       
                       <br>                       
                      Video:<a href="https://youtu.be/VeuRKfGhZqA"> Link</a>
                       <br>
                     Link:<a href="https://github.com/rohiitb/rebar_intersection_detection"> Github</a>
                       <p></p>
                       <p>
                         In this project, we developed a novel algorithm for detecting rebar intersections real-time in a fast manner in a densely multilayered rebar network which are used in construction sites.<br>
                         Intel Realsense RGB-D camera was used to obtain the pointcloud. Planar segmentation was done to get the topmost layer of the rebar 
                         and independent rebars were identified followed by rebar intersections. Pose detection was done on the rebar intersection to find out the direction of approach of the quadraped robot.
                         Additionaly, each rebar intersection was mapped in the global frame with a unique id. <br>
                         We also worked on treating the intersections as landmarks to perform graph optimization using g2o, gtsam packages and minimize the drift in the odometry of the robot.<br>
                         I also worked on generating pointclouds from multiple photos taken with a drone using photogrammetry and increasing the confidence score of the rebars in the pointclouds.
                         
                       <br>
                       </p>
                     </td>
                   </tr>
        
         
            

          <!-- Quadrotor -->

          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()">  
            <td style="padding:20px;width:30%;vertical-align:middle">
             <div class="one">
                 <img src='images/crazyflie.gif' width="200" height="210">
             </div>

              <script type="text/javascript">
                function nerf_start() {
                  document.getElementById('nerf_image').style.opacity = "1";
                }

                function nerf_stop() {
                  document.getElementById('nerf_image').style.opacity = "0";
                }
                nerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>
                  Quadrotor Planning and Control 
                </papertitle>
              <br>
        <em>Spring 2022</em>
              <br>
              Project page:<a href="data/quad_controller.pdf"> Link</a>
              <br>

              Report:<a href="data/proj_report.pdf"> Link</a>
              <br>
              Video:<a href="https://youtu.be/hxh-_XgXjow"> Link</a>
              <br>
              Code:<a href="https://github.com/rohiitb/Quadrotor_planning_and_control"> Github</a>
              <p></p>
              <p> In this project, we implemented Geometric non-linear controller algorithm to control a
                  quadrotor which could follow a predefined trajectory and reach the goal without collision in an obstacle cluttered environment. 
                  The optimal trajectory was obtained by down-sampling the waypoints to the shortest path which was
                  generated by A*, followed by trajectory smoothening.
                <br>
                  We also fused the IMU and stereo pair information to get the 3D pose of the quadrotor using Error State Kalman Filter (Visual Inertial Odometry). 
                <br>
                  This project focused on demonstrating the planning aspect of robot capabilities. During the lab, the
                  CrazyFlie 2.0 was used for realistic demonstrations. A
                  microcomputer is responsible for low-level control and
                  estimation, while the onboard IMU provides feedback
                  of angular velocities and accelerations. The attitude
                  and thrust commands are sent to the quadrotor via the
                  CrazyRadio after being computed in python.
                  <br>
              </p>
            </td>
          </tr> 

          
          <!-- F1 tenth -->

<!--          <tr onmouseout="porshadmanip_stop()" onmouseover="porshadmanip_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/race.gif' width="220" height="220">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://f1tenth.org/">
                <papertitle>F1tenth Racing (UPenn ESE 615 Autonomous Racing)</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/shivangimisra1/">Shivangi Misra</a>,
              <strong>Dingding Zheng</strong>,
              <a href="https://weiyi-tang.netlify.app/">Weiyi Tang</a>
              <br>
              <em>Spring</em> 2020
              <br>
              <a href="https://f1tenth.org/">project page</a> /
              
              <a href="data/ESE_615_Final_Report_by_Team_1.pdf">report</a>/

              <a href="https://github.com/zddkjmuner/Autonomous-Racing">code</a>
              <p></p>
              <p><a href="https://f1tenth.org/build.html">”F1tenth”</a> is an open-source, small-scale racing car platform
                widely used for teaching and research in safe autonomy.
                Maneuvering a racing car to finish loops in minimum time
                has been studied for decades. However, it’s always computationally
                expensive and infeasible to solve this problem by
                real-time trajectory planning. In this project, we generated
                a velocity profile to find the maximum permissible speed of
                racing car on each waypoint on the path. Then, we use CMAES
                (Covariance Matrix Adaptation - Evolution Strategy) to
                generate the desired path for vehicle to track. The generated
                path has a relatively small curvature which allows the racing
                car to run at high, steady speed. Pure pursuit is used as
                the vehicle controller. To avoid obstacles, we implemented
                ODG-PFM (Obstacle-Dependent Gaussian Potential
                Field) and compared its performance VS. RRT*. In order to
                measure the performance of these two algorithms, we setup
                several testing maps and added noise to the environment.
              </p>
            </td>
          </tr>    -->

  
          <!-- SLAM -->
  
          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()">  <!--bgcolor="#ffffd0"-->
                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                          <img src='images/map1.PNG' width="200" height="200">
                        </div>
                        <script type="text/javascript">
                          function nerf_start() {
                            document.getElementById('nerf_image').style.opacity = "1";
                          }

                          function nerf_stop() {
                            document.getElementById('nerf_image').style.opacity = "0";
                          }
                          nerf_stop()
                        </script>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                          <papertitle>Simultaneous Localization and Mapping (SLAM)
                          </papertitle>
                        <br>
                      <em>Spring 2022</em>
                        <br>
                        Code:<a href="https://github.com/rohiitb/MonteCarlo_localization_SLAM"> Github</a>
                        <p></p>
                        <p>
                          In this project, we integrated the IMU orientation and the odometry information obtained from 2D LiDAR scans from a walking humanoid robot in order to build a 2D occupancy grid map of the walls and obstacles in the environment. <br>
                            <br>
                        </p>
                      </td>
                    </tr>
          
          
          <!-- MPC Manipulator Arm -->
  
          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()">  <!--bgcolor="#ffffd0"-->
                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                          <img src='images/mpc_arm_gif.gif' width="200" height="200">
                        </div>
                        <script type="text/javascript">
                          function nerf_start() {
                            document.getElementById('nerf_image').style.opacity = "1";
                          }

                          function nerf_stop() {
                            document.getElementById('nerf_image').style.opacity = "0";
                          }
                          nerf_stop()
                        </script>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                          <papertitle>7-DoF Robotic Arm Manipulation using MPC and Vision-based Object grasping
                          </papertitle>
                        <br>
                      <em>Fall 2022</em>
                        <br>
                        Report:<a href="data/mpc_manipulator_report.pdf"> Link</a>
                        <br>
                        Code:<a href="https://github.com/rohiitb/manipulator_mpc"> Github</a>
                        <p></p>
                        <p>
                            In this project, for the first stage, we succesfully solved the task of picking up static and dynamic cubes. Cubes were having Apriltags on their faces which were used to get the pose. <br>
                            Implemented forward kinematics, inverse kinematics to plan a path to pick up the cubes and stack them to create a tower.<br>
                            Implemented Rapidly Exploring Random Trees (RRT), Potential fields algorithm in ROS for 7-DoF FrankaPanda Arm to plan a path to pick up the cubes.<br>
                            For the second stage, we implemented a finite horizon Model Predictive Control (MPC) with position, velocity, acceleration control with the waypoints given by RRT algorithm.<br>
                            For the third stage, we implemented Heirarchical MPC algorithm where MPC was optimizing the path given by RRT and choosing the path with least cost.  
                            <br>
                        </p>
                      </td>
                    </tr>

          
          <!-- UKF -->
          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()">  <!--bgcolor="#ffffd0"-->
            <td style="padding:20px;width:25%;vertical-align:middle">
             <div class="one">
               <img src='images/res1.png' width="270" height="200">
              </div>
              <script type="text/javascript">
                function nerf_start() {
                  document.getElementById('nerf_image').style.opacity = "1";
                }

                function nerf_stop() {
                  document.getElementById('nerf_image').style.opacity = "0";
                }
                nerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Orienation Tracking using Unscented Kalman Filter</papertitle>
              <br>
                <em>Spring 2022</em>
                <br>
              Code:<a href="https://github.com/rohiitb/Orientation_tracking_using_UKF"> Github</a>
              <p></p>
              <p>
                  In this project, we implemented an Unscented Kalman filter to track three dimensional orientation.<br>
                  From the given IMU sensor readings from gyroscopes and accelerometers, we estimated the underlying 3D orientation by learning the appropriate model parameters from ground truth data given by a Vicon motion capture system.
                <br>
              </p>
            </td>
          </tr>

           <!--         Mask-RCNN -->

                   <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"> 
                     <td style="padding:20px;width:30%;vertical-align:middle">
                      <div class="one">
                          <img src='images/mh1.png' width="200" height="200">
                      </div>
                       
                       <script type="text/javascript">
                         function nerf_start() {
                           document.getElementById('nerf_image').style.opacity = "1";
                         }

                         function nerf_stop() {
                           document.getElementById('nerf_image').style.opacity = "0";
                         }
                         nerf_stop()
                       </script>
                     </td>
                     
                     
                     <td style="padding:20px;width:75%;vertical-align:middle">
                       <papertitle>
                         Mask-RCNN (Instance Segmentation) 
                        </papertitle>
                       <br>
                 <em>Fall 2022</em>
                       <br>                       
                      Code:<a href="https://github.com/rohiitb/MaskRCNN"> Github</a>
                       <p></p>
                       <p>
                       In this project, Mask-RCNN object detection and instance segmentation pipeline is implemented and tested on COCO dataset consisting of 3 object classes - Animals, Vehicles and People.
                       <br>
                       <br>
                       Mask RCNN is a a conceptually simple, flexible, and general framework for object instance segmentation.It extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. This pipeline was implemented to do per pixel object detection and the corresponding mask.
                       <br>
                       </p>
                     </td>
                   </tr>
          
          <!-- YOLO -->

          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/yolo.JPG' width="190">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>YOLO (Object Detection)</papertitle>
              <br>
                <em>Fall</em> 2022
              <br>
                Code:<a href="https://github.com/rohiitb/YOLO"> Github</a>
              <p></p>
              <p>Implemented YOLO pipeline from scratch to do object detection on street scene images. <br>MAP acheived : 0.43</p>
            </td>
          </tr>
          
          <!-- SOLO -->

          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/first1.png' width="190">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>SOLO (Instance Segmentation)</papertitle>
              <br>
                <em>Fall</em> 2022
              <br>
                Code:<a href="https://github.com/rohiitb/SOLO_Instance_segmentation"> Github</a>
              <p></p>
              <p>Implemented a single-stage instance segmentation pipeline "Segmenting Objects by Location" based on Feature pyramid network model to classify and generate masks for objects belonging to 3 categories: Animals, Vehicles and Humans.</p>
            </td>
          </tr>
          
          

            <!-- 3D Reconstruction from images -->
            <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
              <td style="padding:30px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/recon_gif.gif' width="200" height="160">
                </div>
                <script type="text/javascript">
                  function dpzlearn_start() {
                    document.getElementById('dpzlearn_image').style.opacity = "1";
                  }

                  function dpzlearn_stop() {
                    document.getElementById('dpzlearn_image').style.opacity = "0";
                  }
                  dpzlearn_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
            
                  <papertitle>3D Reconstruction from images </papertitle>
                <br>
                    <em>Spring 2022</em>
                <br>
              Code:<a href="https://github.com/rohiitb/Two_view_stereo_3D_reconstruction"> Github</a>
                <br>
                <p>
                  In this project, we were given multiple images of a scene. We reconstructed the 3D pointcloud of the scene using 2 algorithms:<br>
                  <em>Two-View Stereo</em>: We generated the disparity map using distance metric kernels such as ssd, sad and zncc which gave the depth map of the scene.<br>
                  <em>Multiview Stereo</em>: We constructed a cost volume by stacking depth cost maps obtained by projecting reference view and warped neighboring view using homography.
                </p>
              </td>
            </tr>
          
          
          <!-- Autonomous Battle Bot -->
          <tr onmouseout="loss_stop()" onmouseover="loss_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/pic2.jpg' width="200" height="220">
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }

                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Autonomous Battle Bot</papertitle>
              <br>
              <em>Fall</em> 2021
              <br>
              Code: <a href="https://github.com/rohiitb/Autonomous_Battle_bot">Github</a>
              <p></p>
              <p>
                Designed and fabricated a 2-wheel differential drive robot with a gripper arm capable of grabbing cans and also detect light of desired frequencies.<br>
                Robot was also capable of navigating itself to a particular location using VIVE localization system.<br>
                ESP32 microcontroller was used for controlling and grabbing.<br>
                Wireless communication via HTML(Webpage) and UDP was used to send commands to the robot.
              </p>
            </td>
          </tr>







        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                    Great thanks to Jon for his amazing <a href="https://jonbarron.info/">website template</a>!

              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>


<!--Great thanks to Jon for his amazing <a href="https://jonbarron.info/">website template</a>!-->
</body>

</html>
